# src/model.py

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import joblib
import mlflow
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score

# Import custom feature engineering transformers
from src.features import FeatureCombiner, FraudRateCalculator, HighRiskIndicator

def train_and_log_model(df: pd.DataFrame, mlflow_tracking_uri: str, experiment_name: str, best_threshold: float):
    """
    Trains the XGBoost model, logs metrics to MLflow, and saves the pipeline and model.

    Args:
        df (pd.DataFrame): The input DataFrame containing the data.
        mlflow_tracking_uri (str): The MLflow tracking URI.
        experiment_name (str): The name of the MLflow experiment.
        best_threshold (float): The chosen threshold for prediction.
    """
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    mlflow.set_experiment(experiment_name)

    # Define features (X) and target (y)
    X = df.drop('label', axis=1)
    y = df['label']

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)

    # --- Feature Engineering and Preprocessing Pipeline ---
    # This pipeline includes the custom feature engineering steps and one-hot encoding.
    # SMOTE is applied separately to the training data after this pipeline.

    # Define preprocessing steps for numerical and categorical features
    numerical_features = ['amount'] # 'fraud_rate_combo' will be generated by the pipeline
    categorical_features = ["merchant_type", "device_type", "amount_category"] # These are used in feature engineering

    # Create transformers for categorical features
    # The FeatureCombiner and FraudRateCalculator will add 'merchant_device_only' and 'fraud_rate_combo'
    # OneHotEncoder will then be applied to the relevant categorical features *after* feature engineering.
    # We need to carefully construct the pipeline to ensure features are available when needed.

    # Let's define the full preprocessing pipeline including feature engineering and encoding
    full_preprocessing_pipeline = Pipeline([
        ('combiner', FeatureCombiner()),
        ('fraud_rate_calc', FraudRateCalculator()),
        ('high_risk_indicator', HighRiskIndicator(quantile_threshold=0.85)),
        ('column_transformer', ColumnTransformer(
            transformers=[
                ('cat', OneHotEncoder(handle_unknown='ignore'), ["amount_category", "merchant_device_only"]) # One-hot encode the relevant features *after* creation
            ],
            remainder='passthrough' # Keep other columns like 'amount', 'fraud_rate_combo', 'is_high_risk_combo'
        ))
    ])

    # Fit and transform the training data using the full preprocessing pipeline
    X_train_processed = full_preprocessing_pipeline.fit_transform(X_train, y_train)

    # Apply SMOTE to the processed training data
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X_train_processed, y_train)

    # --- Model Training ---
    # Calculate scale_pos_weight for XGBoost
    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

    # Initialize and train the XGBoost model
    model = XGBClassifier(
        scale_pos_weight=scale_pos_weight,
        objective="binary:logistic",
        random_state=42,
        eval_metric="aucpr"
    )
    model_xgboost = model.fit(X_resampled, y_resampled)

    # --- Evaluation and MLflow Logging ---
    with mlflow.start_run():
        mlflow.log_param("best_threshold", best_threshold)
        mlflow.log_param("smote_random_state", 42)
        mlflow.log_param("xgb_scale_pos_weight", scale_pos_weight)

        # Transform the test data using the fitted preprocessing pipeline (without SMOTE)
        X_test_processed = full_preprocessing_pipeline.transform(X_test)

        # Predict probabilities on the test set
        y_proba = model_xgboost.predict_proba(X_test_processed)[:, 1]

        # Apply the best threshold to get binary predictions
        y_pred = (y_proba >= best_threshold).astype(int)

        # Calculate evaluation metrics
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)

        print(f"Metrics for threshold {best_threshold:.2f}:")
        print(f"Precision: {precision:.2f}")
        print(f"Recall: {recall:.2f}")
        print(f"F1-Score: {f1:.2f}")
        print("Confusion Matrix:")
        print(cm)

        # Log metrics to MLflow
        mlflow.log_metric("test_precision", precision)
        mlflow.log_metric("test_recall", recall)
        mlflow.log_metric("test_f1_score", f1)
        # You can log confusion matrix as an artifact if needed

        # Log the full preprocessing pipeline and the trained model
        # We save the full pipeline up to the point before the model for inference.
        # The model is saved separately.
        joblib.dump(full_preprocessing_pipeline, '../models/full_preprocessing_pipeline.pkl')
        joblib.dump(model_xgboost, '../models/best_xgboost_model.pkl')

        mlflow.log_artifact('../models/full_preprocessing_pipeline.pkl')
        mlflow.log_artifact('../models/best_xgboost_model.pkl')

    print("Model training, logging, and saving complete.")

if __name__ == '__main__':
    # Example usage (assuming df is loaded in the notebook)
    # You would call this function from your notebook or another script
    # train_and_log_model(df, "https://103-150-90-187.sslip.io/mlflow", "fraud_detection_api_training", 0.88)
    print("model.py is intended to be imported and run, not executed directly.")
    print("Load your data and call the 'train_and_log_model' function.")